{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "O0b109SDkzMO"
      },
      "outputs": [],
      "source": [
        "!git clone \"https://github.com/anoopkunchukuttan/indic_nlp_library\"\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git\n",
        "!pip install Morfessor\n",
        "INDIC_NLP_LIB_HOME=r\"/content/indic_nlp_library\"\n",
        "INDIC_NLP_RESOURCES=\"/content/indic_nlp_resources\"\n",
        "import sys\n",
        "sys.path.append(r'{}'.format(INDIC_NLP_LIB_HOME))\n",
        "from indicnlp import common\n",
        "common.set_resources_path(INDIC_NLP_RESOURCES)\n",
        "from indicnlp import loader\n",
        "loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5XHMm1pvjW3H"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random \n",
        "from tqdm import tqdm "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGYkDbSllnZz",
        "outputId": "5acc0989-22c9-43ec-92e8-b634dadb021e"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "2Dn3DRYylpLl"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "data_hindi=[['hindi']]\n",
        "data_english=[['english']]\n",
        "with open('/content/drive/MyDrive/eng_Hindi_data_train.csv','r') as file:\n",
        "  my_file=csv.reader(file,delimiter=',')\n",
        "  for row in my_file:\n",
        "    data_hindi.append([row[1]])\n",
        "    data_english.append([row[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyVsSw0Jl-Kc",
        "outputId": "1476cb39-f848-4602-910e-28998915615f"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "lem=WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Xvm2rda9mE8E"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "def decontract(sentence):\n",
        "  sentence=sentence[0].split()\n",
        "  sentence=' '.join(sentence)\n",
        "  sentence=re.sub(r\"won't\",'will not',sentence)\n",
        "  sentence=re.sub(r\"can't\",'can not',sentence)\n",
        "  sentence=re.sub(r\"n\\'t\",' not',sentence)\n",
        "  sentence=re.sub(r\"\\'re\",' are',sentence)\n",
        "  sentence=re.sub(r\"\\'d\",' would',sentence)\n",
        "  sentence=re.sub(r\"\\'ll\",' will',sentence)\n",
        "  sentence=re.sub(r\"\\'s\",' is',sentence)\n",
        "  sentence=re.sub(r\"\\'m\",' am',sentence)\n",
        "  sentence=re.sub(r\"\\'ve\",' have',sentence)\n",
        "  return sentence.split(' ')\n",
        "  \n",
        "def cleanEng(x):\n",
        "  x=' '.join(x)\n",
        "  x=str(x)\n",
        "  x=x.lower()\n",
        "  x=re.sub(r'[^a-z0-9]+',' ',x)\n",
        "  x=re.sub(' +', ' ',x)\n",
        "  if len(x)>200:\n",
        "    x=x[:200]\n",
        "  if x and x[-1]==' ':\n",
        "    x=x[:-1]\n",
        "  x=x.split(' ')\n",
        "  return x\n",
        "\n",
        "def cleanHindi(x):\n",
        "  x=' '.join(x)\n",
        "  x=str(x)\n",
        "  x=re.sub('[a-zA-Z]','',x)\n",
        "  translator=str.maketrans('', '', string.punctuation)\n",
        "  x=x.translate(translator)\n",
        "  x=re.sub(' +', ' ',x)\n",
        "  if len(x)>200:\n",
        "    x=x[:200]\n",
        "  if x and x[-1]==' ':\n",
        "    x=x[:-1]\n",
        "  x=x.split(' ')\n",
        "  return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "30hqSAMimOll"
      },
      "outputs": [],
      "source": [
        "for i in range(1,len(data_english)):\n",
        "  data_english[i]=list(decontract(data_english[i]))\n",
        "  data_english[i]=cleanEng(data_english[i])\n",
        "  for j in range(len(data_english[i])):\n",
        "    data_english[i][j]=lem.lemmatize(data_english[i][j])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "ljbscjKOmTgg"
      },
      "outputs": [],
      "source": [
        "from indicnlp.normalize.indic_normalize import BaseNormalizer\n",
        "remove_nuktas=False\n",
        "normalizer=BaseNormalizer(\"hi\",remove_nuktas=False)\n",
        "for i in range(1,len(data_hindi)):\n",
        "  data_hindi[i]=cleanHindi(data_hindi[i])\n",
        "  x=' '.join(data_hindi[i])\n",
        "  x=str(x)\n",
        "  x=normalizer.normalize(x)\n",
        "  x=x.split(' ')\n",
        "  data_hindi[i]=x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "ZMheCsMtHhZ-"
      },
      "outputs": [],
      "source": [
        "clean_data_english=data_english\n",
        "clean_data_hindi=data_hindi\n",
        "unique_hin=set()\n",
        "unique_eng=set()\n",
        "dict_eng={}\n",
        "dict_hin={}\n",
        "\n",
        "for i in range(len(clean_data_hindi)):\n",
        "  for j in range(len(clean_data_hindi[i])):\n",
        "    if clean_data_hindi[i][j] not in unique_hin:\n",
        "      unique_hin.add(clean_data_hindi[i][j])\n",
        "      dict_hin[clean_data_hindi[i][j]]=1\n",
        "    else:\n",
        "      dict_hin[clean_data_hindi[i][j]]+=1\n",
        "\n",
        "for i in range(len(clean_data_english)):\n",
        "  for j in range(len(clean_data_english[i])):\n",
        "    if clean_data_english[i][j] not in unique_eng:\n",
        "      unique_eng.add(clean_data_english[i][j])\n",
        "      dict_eng[clean_data_english[i][j]]=1\n",
        "    else:\n",
        "      dict_eng[clean_data_english[i][j]]+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "PahUHrtXH3ze"
      },
      "outputs": [],
      "source": [
        "dict_eng=sorted(dict_eng.items(),key=lambda p:p[1],reverse=True)  \n",
        "dict_hin=sorted(dict_hin.items(),key=lambda p:p[1],reverse=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "rThMOKfxIYqv"
      },
      "outputs": [],
      "source": [
        "word_count_hin=[]\n",
        "word_count_eng=[]\n",
        "\n",
        "for i in range(len(clean_data_hindi)):\n",
        "  word_count_hin.append(len(clean_data_hindi[i]))\n",
        "  word_count_eng.append(len(clean_data_english[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "fFVlthn9mZyn"
      },
      "outputs": [],
      "source": [
        "def addTokens(x,start=False):\n",
        "  x.append('<END>')\n",
        "  if start:\n",
        "    x.insert(0,'<START>')\n",
        "  return list(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "0BhjwvfnmgUI"
      },
      "outputs": [],
      "source": [
        "for i in range(len(clean_data_english)):\n",
        "  clean_data_english[i]=addTokens(clean_data_english[i],start=True)\n",
        "  clean_data_hindi[i]=addTokens(clean_data_hindi[i],start=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "Pe6m-RPamp5r"
      },
      "outputs": [],
      "source": [
        "validate_eng=clean_data_english[81858:]\n",
        "validate_hin=clean_data_hindi[81858:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "6t6ULt-_mrfz"
      },
      "outputs": [],
      "source": [
        "class vocab:\n",
        "  def __init__(self,data,token=True):\n",
        "    self.data=data\n",
        "    if token:\n",
        "      self.word2idx={'<START>':1,'<END>':2,'<PAD>':0}\n",
        "      self.idx2word={1:'<START>',2:'<END>',0:'<PAD>'}\n",
        "      self.idx=2\n",
        "\n",
        "    else:\n",
        "      self.word2idx={'<PAD>':0,'<END>':1}\n",
        "      self.idx2word={0:'<PAD>',1:'<END>'}\n",
        "      self.idx=1\n",
        "\n",
        "    self.x=[]\n",
        "    self.create()\n",
        "    self.vocab_size=self.idx+1\n",
        "\n",
        "  def create(self):\n",
        "    max_len=0\n",
        "    for sentence in  self.data:\n",
        "      max_len=max(max_len,len(sentence))\n",
        "      for word in sentence:\n",
        "        if self.word2idx.get(word) is None:\n",
        "          self.idx+=1\n",
        "          self.word2idx[word]=self.idx\n",
        "          self.idx2word[self.idx]=word\n",
        "    \n",
        "    for sentence in self.data:\n",
        "      sent=[]\n",
        "      for word in sentence:\n",
        "        sent.append(self.word2idx[word])\n",
        "      for i in range(len(sentence),max_len+1):\n",
        "        sent.append(0)\n",
        "      self.x.append(torch.Tensor(sent))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "YKJJAF7hmwTq"
      },
      "outputs": [],
      "source": [
        "English_vocab=vocab(clean_data_english[1:],token=True)\n",
        "Hindi_vocab=vocab(clean_data_hindi[1:],token=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "PtNZ49N-my1-"
      },
      "outputs": [],
      "source": [
        "class parallelData(Dataset):\n",
        "  def __init__(self):\n",
        "    self.x=Hindi_vocab.x\n",
        "    self.y=English_vocab.x\n",
        "  def __getitem__(self,i):\n",
        "    return self.x[i],self.y[i]\n",
        "  def __len__(self):\n",
        "    return len(self.x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "mXiwIRZRm2BM"
      },
      "outputs": [],
      "source": [
        "dataset=parallelData() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "vFVnMSPDm4IA"
      },
      "outputs": [],
      "source": [
        "class encoder(nn.Module):\n",
        "\n",
        "  def __init__(self,input_size,embedding_size,hidden_size,layers,bidirectional,p):\n",
        "    super().__init__()\n",
        "    self.embed=nn.Embedding(num_embeddings=input_size,embedding_dim=embedding_size)\n",
        "    self.lstm=nn.LSTM(input_size=embedding_size,hidden_size= hidden_size,num_layers=layers,batch_first=True,bidirectional=bidirectional)\n",
        "    self.bidirectional=bidirectional\n",
        "    self.dropout=nn.Dropout(p)\n",
        "    self.fc_hidden=nn.Linear(hidden_size*2,hidden_size)\n",
        "    self.fc_cell=nn.Linear(hidden_size*2,hidden_size)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x=self.dropout(self.embed(x))\n",
        "    output,(hidden_state,cell_state)=self.lstm(x)\n",
        "\n",
        "    if self.bidirectional:\n",
        "      hidden=torch.cat((hidden_state[0:1],hidden_state[1:2]),dim=2)\n",
        "      cell=torch.cat((cell_state[0:1],cell_state[1:2]),dim=2)\n",
        "      hidden_state = self.fc_hidden(hidden)\n",
        "      cell_state = self.fc_cell(cell)\n",
        "\n",
        "    return output,hidden_state,cell_state\n",
        "\n",
        "class decoder(nn.Module):\n",
        "\n",
        "  def __init__(self,input_size,embedding_size,hidden_size,layers,p):\n",
        "\n",
        "    super().__init__()\n",
        "    self.embed=nn.Embedding(num_embeddings=input_size,embedding_dim=embedding_size)\n",
        "    self.dropout=nn.Dropout(p)\n",
        "    self.lstm=nn.LSTM(input_size=embedding_size, hidden_size= hidden_size, num_layers=layers, batch_first = True)\n",
        "    self.fc=nn.Linear(in_features=hidden_size, out_features=input_size)\n",
        "\n",
        "  def forward(self,x,hidden_state, cell_state):\n",
        "    \n",
        "    x=x.reshape(-1,1)\n",
        "    x=self.dropout(self.embed(x))\n",
        "    output,(hidden_state, cell_state)=self.lstm(x,(hidden_state, cell_state))\n",
        "    output=self.fc(output)\n",
        "    output=output.squeeze(dim=1)\n",
        "\n",
        "    return output,hidden_state,cell_state\n",
        "    \n",
        "class AttnDecoder(nn.Module):\n",
        "  def __init__(self, input_size, embedding_size, hidden_size, layers):\n",
        "\n",
        "    super().__init__()\n",
        "    self.embed=nn.Embedding(num_embeddings=input_size, embedding_dim=embedding_size)\n",
        "    self.lstm=nn.LSTM(input_size=hidden_size*2+embedding_size,hidden_size=hidden_size,num_layers=layers,batch_first=True)\n",
        "    self.fc=nn.Linear(in_features=hidden_size,out_features=input_size)\n",
        "    self.energy=nn.Linear(hidden_size*3,1)\n",
        "    self.softmax=nn.Softmax(dim=1)\n",
        "  \n",
        "  def forward(self,x,hidden_state,cell_state,encoder_states):\n",
        "\n",
        "    batch_size=encoder_states.shape[0]\n",
        "    seq_len=encoder_states.shape[1]\n",
        "    hidden_size=encoder_states.shape[2]\n",
        "\n",
        "    h_new=hidden_state.repeat(seq_len,1,1) \n",
        "    h_new=h_new.permute(1,0,2)\n",
        "\n",
        "    energy=self.energy(torch.cat((h_new, encoder_states), dim=2))\n",
        "    att_weights=self.softmax(energy)\n",
        "    att_weights=att_weights.permute(0,2,1) \n",
        "    context=torch.bmm(att_weights, encoder_states) \n",
        "\n",
        "    x=x.reshape(-1,1) \n",
        "    x=self.embed(x)\n",
        "\n",
        "    input_new=torch.cat((context,x), dim=2)\n",
        "\n",
        "    output,(hidden_state, cell_state)=self.lstm(input_new,(hidden_state, cell_state))\n",
        "    output=self.fc(output) \n",
        "    output=output.squeeze(dim=1) \n",
        "\n",
        "    del input_new\n",
        "    del context\n",
        "    del h_new\n",
        "\n",
        "    return output,hidden_state,cell_state\n",
        "\n",
        "class Attnseq2seq(nn.Module):\n",
        "  def __init__(self, encoder, att_decoder):\n",
        "\n",
        "    super().__init__()\n",
        "    self.encoder=encoder\n",
        "    self.decoder=att_decoder\n",
        "\n",
        "  def forward(self, input, target, teaching_force=0.6):\n",
        "    batch_size =input.shape[0]\n",
        "    seq_len=target.shape[1]\n",
        "    eng_vocab_size=English_vocab.vocab_size\n",
        "    output=torch.zeros((seq_len, batch_size, eng_vocab_size)).to(device)\n",
        "    encoder_states,hidden, cell=self.encoder(input)\n",
        "    target=target.permute(1,0) \n",
        "    x=target[0] \n",
        "\n",
        "    for i in range(1,seq_len):\n",
        "      out,hidden,cell=self.decoder(x, hidden, cell, encoder_states)\n",
        "      output[i]=out\n",
        "      decoder_guess=out.argmax(1) \n",
        "      if random.random()<teaching_force:\n",
        "        x=target[i]\n",
        "      else:\n",
        "        x=decoder_guess\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "CeTmX39Hm6ME"
      },
      "outputs": [],
      "source": [
        "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_epochs=20\n",
        "learning_rate=0.001\n",
        "batch_size=128\n",
        "embedding_size=512\n",
        "hidden_size=512\n",
        "layers=1\n",
        "bidirectional=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "MZCxglpHm8Xx"
      },
      "outputs": [],
      "source": [
        "loader=DataLoader(dataset,batch_size=batch_size,shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "Zg459vVLm-LT"
      },
      "outputs": [],
      "source": [
        "ENC=encoder(Hindi_vocab.vocab_size,embedding_size,hidden_size,layers,bidirectional,0.1).to(device) \n",
        "\n",
        "DE=AttnDecoder(English_vocab.vocab_size,embedding_size,hidden_size,layers).to(device) \n",
        "\n",
        "model=Attnseq2seq(ENC,DE).to(device)\n",
        "optimizer=optim.Adam(model.parameters(),lr=learning_rate) \n",
        "criterion=nn.CrossEntropyLoss(ignore_index=0) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "wYlra7VMnBEv",
        "outputId": "a5cd5506-1509-4d3a-bc4f-1d9aa579515c"
      },
      "outputs": [],
      "source": [
        "train_loss = []\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "  for id,(x,y) in (enumerate(tqdm(loader))):\n",
        "    x=x.long().to(device)\n",
        "    y=y.long().to(device)\n",
        "    output=model(x,y)\n",
        "    output=output[1:].reshape(-1,output.shape[2])\n",
        "    y=y.permute(1,0)\n",
        "    y=y[1:].reshape(-1)\n",
        "    optimizer.zero_grad()\n",
        "    loss=criterion(output,y)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(),max_norm=1)\n",
        "    optimizer.step()\n",
        "  print(f'[{epoch+1}/{num_epochs}] loss={loss.item()}')\n",
        "  train_loss.append(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iN-v6HSMRT-u"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9QsLSsAd5S3"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(),'/content/drive/MyDrive/model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "339yF-sVEZxr",
        "outputId": "8a3b1bca-8e84-4318-9b91-f9a073743d22"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load('/content/drive/MyDrive/model'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "aKblfha_d_Fl"
      },
      "outputs": [],
      "source": [
        "def prediction(x):\n",
        "  x=x.long().reshape(1,-1).to(device)\n",
        "  ans=translate(x)\n",
        "  res=[]\n",
        "  for id in ans:\n",
        "    res.append(English_vocab.idx2word[id])\n",
        "  return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "poem-W38eDHs"
      },
      "outputs": [],
      "source": [
        "def translate(input):\n",
        "  with torch.no_grad():\n",
        "    guess=[]\n",
        "    encoder_states,hidden,cell=model.encoder(input)\n",
        "    x=torch.ones((1)).long().to(device)\n",
        "    c=0\n",
        "    while True:\n",
        "      out,hidden,cell=model.decoder(x,hidden,cell,encoder_states)\n",
        "      x=out.argmax(1)\n",
        "      c+=1\n",
        "      guess.append(int(x[0].detach().cpu()))\n",
        "      if x == 2 or c>100:\n",
        "        break\n",
        "  return guess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "LfGlaRfZeK_9"
      },
      "outputs": [],
      "source": [
        "def get(sent):\n",
        "  token=[]\n",
        "  for word in sent:\n",
        "    if Hindi_vocab.word2idx.get(word) is None:\n",
        "      token.append(Hindi_vocab.word2idx['है']) \n",
        "    else:\n",
        "      token.append(Hindi_vocab.word2idx[word])\n",
        "  sent=torch.tensor(token).float()\n",
        "  res=prediction(sent)\n",
        "  return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbaBrcxzeSLX",
        "outputId": "62f0fa1f-9aec-42bf-ddbd-867958bcb80e"
      },
      "outputs": [],
      "source": [
        "!pip install -U nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "gTebZLfuh6FL"
      },
      "outputs": [],
      "source": [
        "test_hindi=[['hindi']]\n",
        "with open('/content/drive/MyDrive/eng_Hindi_data_test_X.csv','r') as file:\n",
        "  my_file=csv.reader(file,delimiter=',')\n",
        "  for row in my_file:\n",
        "    test_hindi.append([row[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUq8oWSTie8c",
        "outputId": "f0495fd7-85f3-4725-c8ec-6d16bdb7dc20"
      },
      "outputs": [],
      "source": [
        "test_hindi=test_hindi[1:]\n",
        "print(len(test_hindi))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "gVq-8vnKjjwD"
      },
      "outputs": [],
      "source": [
        "for i in range(len(test_hindi)):\n",
        "  test_hindi[i]=cleanHindi(test_hindi[i])\n",
        "  x=' '.join(test_hindi[i])\n",
        "  x=str(x)\n",
        "  x=normalizer.normalize(x)\n",
        "  x=x.split(' ')\n",
        "  test_hindi[i]=x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "leRsX5juS24u"
      },
      "outputs": [],
      "source": [
        "unique_test=set()\n",
        "dict_test=dict()\n",
        "word_count_test=[]\n",
        "for i in range(len(test_hindi)):\n",
        "  word_count_test.append(len(test_hindi[i]))\n",
        "  for j in range(len(test_hindi[i])):\n",
        "    if test_hindi[i][j] in dict_test:\n",
        "      dict_test[test_hindi[i][j]]+=1\n",
        "    else:\n",
        "      dict_test[test_hindi[i][j]]=1\n",
        "    if test_hindi[i][j] not in unique_hin:\n",
        "       unique_test.add(test_hindi[i][j])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "2YWw1902TBO7"
      },
      "outputs": [],
      "source": [
        "dict_test=sorted(dict_test.items(),key=lambda p:p[1],reverse=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "81uFwCNGj7aK"
      },
      "outputs": [],
      "source": [
        "test_dataset=vocab(test_hindi,token=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqMJg0oIkiYP",
        "outputId": "f8e37e6e-66dc-42e9-ff67-f459f4dbfbcc"
      },
      "outputs": [],
      "source": [
        "pred=[]\n",
        "actu=[]\n",
        "for i in tqdm(range(len(test_hindi))):\n",
        "  k=get(test_hindi[i])[:-1]\n",
        "  q=test_hindi[i][:-1]\n",
        "  actu.append(q)\n",
        "  pred.append(k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "OeQGLkBIlWRB",
        "outputId": "2c36ed90-fd8d-4e29-a524-ba7a91045359"
      },
      "outputs": [],
      "source": [
        "f = open(\"/content/drive/MyDrive/answer.txt\",\"w\")\n",
        "for i in range(len(pred)):\n",
        "  s=' '.join(pred[i])\n",
        "  s+='\\n'\n",
        "  f.write(s)\n",
        "f.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
